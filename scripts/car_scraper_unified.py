#!/usr/bin/env python3
"""
CarScraper Unified - Zjednoten√Ω syst√©m s viacer√Ωmi zdrojmi a fallback logikou
Kombinuje v≈°etky zdroje a zabezpeƒçuje redundanciu
"""

import sys
import os
import time
import logging
from typing import List, Dict, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FutureTimeout

# Pridaj parent adres√°r do path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import jednotliv√Ωch scraperov
try:
    from scripts.car_scraper_bazos import scrape_bazos
    BAZOS_AVAILABLE = True
except ImportError:
    BAZOS_AVAILABLE = False
    print("‚ö†Ô∏è Bazo≈° scraper nie je dostupn√Ω")

try:
    from scripts.car_scraper_autobazar import scrape_autobazar
    AUTOBAZAR_AVAILABLE = True
except ImportError:
    AUTOBAZAR_AVAILABLE = False
    print("‚ö†Ô∏è Autobazar scraper nie je dostupn√Ω")

try:
    from scripts.car_scraper_autosme import scrape_autosme
    AUTOSME_AVAILABLE = True
except ImportError:
    AUTOSME_AVAILABLE = False
    print("‚ö†Ô∏è Auto.sme scraper nie je dostupn√Ω")

logger = logging.getLogger(__name__)

class UnifiedCarScraper:
    """
    Zjednoten√Ω scraper s viacer√Ωmi zdrojmi a inteligentn√Ωm fallback syst√©mom
    """
    
    def __init__(self):
        self.sources = []
        
        # Zoznam dostupn√Ωch zdrojov v porad√≠ priority
        if BAZOS_AVAILABLE:
            self.sources.append({
                'name': 'Bazo≈°.sk',
                'function': scrape_bazos,
                'priority': 1,  # Najvy≈°≈°ia priorita
                'timeout': 20,
                'enabled': True
            })
        
        if AUTOBAZAR_AVAILABLE:
            self.sources.append({
                'name': 'Autobazar.eu',
                'function': scrape_autobazar,
                'priority': 2,
                'timeout': 20,
                'enabled': True  # Enabled for Cheap Cars (<5000 EUR)
            })
        
        if AUTOSME_AVAILABLE:
            self.sources.append({
                'name': 'Auto.sme.sk',
                'function': scrape_autosme,
                'priority': 3,
                'timeout': 20,
                'enabled': False  # DISABLED: URL returns 404 (site structure changed 2026-01)
            })
        
        # Zoradi≈• podƒæa priority
        self.sources.sort(key=lambda x: x['priority'])
        
        logger.info(f'‚úÖ Unified scraper inicializovan√Ω s {len(self.sources)} zdrojmi')
    
    def scrape_single_source(self, source: Dict, search_query: str = "octavia", 
                             min_price: int = 1000, max_price: int = 30000) -> Tuple[str, List[Dict], bool]:
        """
        Scrapuje jeden zdroj
        
        Returns:
            Tuple[str, List[Dict], bool]: (source_name, listings, success)
        """
        if not source['enabled']:
            return (source['name'], [], False)
        
        try:
            print(f"üîÑ [{source['name']}] Sp√∫≈°≈•am scraping...")
            start_time = time.time()
            
            # Spusti scraping s timeoutom
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(
                    source['function'], 
                    search_query, 
                    min_price, 
                    max_price
                )
                try:
                    listings = future.result(timeout=source['timeout'])
                    elapsed = time.time() - start_time
                    
                    if listings and len(listings) > 0:
                        print(f"‚úÖ [{source['name']}] √öspe≈°ne z√≠skan√Ωch {len(listings)} inzer√°tov za {elapsed:.2f}s")
                        return (source['name'], listings, True)
                    else:
                        print(f"‚ö†Ô∏è [{source['name']}] ≈Ωiadne inzer√°ty (alebo pr√°zdny v√Ωsledok)")
                        return (source['name'], [], False)
                except FutureTimeout:
                    print(f"‚è±Ô∏è [{source['name']}] Timeout po {source['timeout']}s")
                    return (source['name'], [], False)
                except Exception as e:
                    print(f"‚ùå [{source['name']}] Chyba: {e}")
                    return (source['name'], [], False)
        
        except Exception as e:
            logger.error(f"Chyba pri scraping {source['name']}: {e}", exc_info=True)
            return (source['name'], [], False)
    
    def scrape_all_parallel(self, search_query: str = "octavia", 
                           min_price: int = 1000, max_price: int = 30000,
                           min_sources: int = 1) -> Dict[str, any]:
        """
        Scrapuje v≈°etky zdroje paralelne
        
        Args:
            search_query: Vyhƒæad√°vac√≠ dotaz
            min_price: Minim√°lna cena
            max_price: Maxim√°lna cena
            min_sources: Minim√°lny poƒçet √∫spe≈°n√Ωch zdrojov (fallback logika)
        
        Returns:
            Dict s v√Ωsledkami: {
                'success': bool,
                'total_listings': int,
                'unique_listings': int,
                'sources_used': List[str],
                'sources_failed': List[str],
                'listings': List[Dict],
                'stats': Dict
            }
        """
        print(f"\n{'='*60}")
        print(f"üöó UNIFIED SCRAPER - Sp√∫≈°≈•am v≈°etky zdroje paralelne")
        print(f"{'='*60}\n")
        
        all_listings = []
        sources_used = []
        sources_failed = []
        source_results = {}
        
        # Spusti v≈°etky zdroje paralelne
        with ThreadPoolExecutor(max_workers=len(self.sources)) as executor:
            futures = {
                executor.submit(
                    self.scrape_single_source, 
                    source, 
                    search_query, 
                    min_price, 
                    max_price
                ): source['name'] 
                for source in self.sources
            }
            
            for future in as_completed(futures):
                source_name = futures[future]
                try:
                    name, listings, success = future.result(timeout=30)
                    
                    if success and listings:
                        sources_used.append(name)
                        all_listings.extend(listings)
                        source_results[name] = {
                            'count': len(listings),
                            'success': True
                        }
                        print(f"‚úÖ [{name}] Pridan√Ωch {len(listings)} inzer√°tov")
                    else:
                        sources_failed.append(name)
                        source_results[name] = {
                            'count': 0,
                            'success': False
                        }
                        print(f"‚ùå [{name}] Zlyhalo")
                
                except Exception as e:
                    source_name = futures[future]
                    sources_failed.append(source_name)
                    source_results[source_name] = {
                        'count': 0,
                        'success': False,
                        'error': str(e)
                    }
                    print(f"‚ùå [{source_name}] V√Ωnimka: {e}")
        
        # Odstr√°≈à duplik√°ty (podƒæa linku)
        unique_listings = []
        seen_links = set()
        
        for listing in all_listings:
            link = listing.get('link', '')
            if link and link not in seen_links:
                seen_links.add(link)
                unique_listings.append(listing)
        
        # Over, ƒçi m√°me dostatok √∫spe≈°n√Ωch zdrojov
        success = len(sources_used) >= min_sources
        
        stats = {
            'total_raw': len(all_listings),
            'unique': len(unique_listings),
            'duplicates_removed': len(all_listings) - len(unique_listings),
            'sources_success': len(sources_used),
            'sources_failed': len(sources_failed),
            'success_rate': len(sources_used) / len(self.sources) * 100 if self.sources else 0
        }
        
        print(f"\n{'='*60}")
        print(f"üìä V√ùSLEDKY:")
        print(f"   √öspe≈°n√© zdroje: {len(sources_used)}/{len(self.sources)}")
        print(f"   Celkom inzer√°tov: {len(all_listings)}")
        print(f"   Unik√°tnych: {len(unique_listings)}")
        print(f"   Duplik√°tov odstr√°nen√Ωch: {stats['duplicates_removed']}")
        print(f"{'='*60}\n")
        
        return {
            'success': success,
            'total_listings': len(all_listings),
            'unique_listings': len(unique_listings),
            'sources_used': sources_used,
            'sources_failed': sources_failed,
            'listings': unique_listings,
            'stats': stats,
            'source_results': source_results
        }
    
    def scrape_with_fallback(self, search_query: str = "octavia", 
                             min_price: int = 1000, max_price: int = 30000) -> List[Dict]:
        """
        Scrapuje s fallback logikou - sk√∫≈°a zdroje jeden po druhom
        
        Returns:
            List[Dict]: Zoznam inzer√°tov
        """
        print(f"\n{'='*60}")
        print(f"üîÑ FALLBACK MODE - Sk√∫≈°am zdroje postupne")
        print(f"{'='*60}\n")
        
        for source in self.sources:
            if not source['enabled']:
                continue
            
            name, listings, success = self.scrape_single_source(
                source, search_query, min_price, max_price
            )
            
            if success and listings:
                print(f"‚úÖ [{name}] √öspe≈°ne z√≠skan√Ωch {len(listings)} inzer√°tov")
                return listings
            else:
                print(f"‚ö†Ô∏è [{name}] Zlyhalo, sk√∫≈°am ƒèal≈°√≠ zdroj...")
                time.sleep(2)  # Kr√°tka pauza medzi zdrojmi
        
        print("‚ùå V≈°etky zdroje zlyhali")
        return []

def scrape_all_sources(search_query: str = "octavia", 
                       min_price: int = 1000, 
                       max_price: int = 30000,
                       mode: str = "parallel") -> List[Dict]:
    """
    Hlavn√° funkcia pre unified scraping
    
    Args:
        search_query: Vyhƒæad√°vac√≠ dotaz
        min_price: Minim√°lna cena
        max_price: Maxim√°lna cena
        mode: "parallel" (v≈°etky naraz) alebo "fallback" (jeden po druhom)
    
    Returns:
        List[Dict]: Zoznam unik√°tnych inzer√°tov
    """
    scraper = UnifiedCarScraper()
    
    if mode == "parallel":
        result = scraper.scrape_all_parallel(search_query, min_price, max_price)
        return result['listings']
    else:
        return scraper.scrape_with_fallback(search_query, min_price, max_price)

if __name__ == '__main__':
    # Test
    results = scrape_all_sources(mode="parallel")
    print(f"\n‚úÖ Celkom z√≠skan√Ωch {len(results)} unik√°tnych inzer√°tov")

